{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Interpreter Lock aka GIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GIL allows only one process to have control on the Python interpreter which means only one process is executed at any point of time. \n",
    "\n",
    "https://realpython.com/python-gil/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single thread, single process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "  'http://www.python.org',\n",
    "  'https://docs.python.org/3/',\n",
    "  'https://docs.python.org/3/whatsnew/3.7.html',\n",
    "  'https://docs.python.org/3/tutorial/index.html',\n",
    "  'https://docs.python.org/3/library/index.html',\n",
    "  'https://docs.python.org/3/reference/index.html',\n",
    "  'https://docs.python.org/3/using/index.html',\n",
    "  'https://docs.python.org/3/howto/index.html',\n",
    "  'https://docs.python.org/3/installing/index.html',\n",
    "  'https://docs.python.org/3/distributing/index.html',\n",
    "  'https://docs.python.org/3/extending/index.html',\n",
    "  'https://docs.python.org/3/c-api/index.html',\n",
    "  'https://docs.python.org/3/faq/index.html'\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = []\n",
    "for url in urls:\n",
    "    with urllib.request.urlopen(url) as src:\n",
    "        results.append(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from concurrent.futures import ThreadPoolExecutor \n",
    "# ThreadPoolExecutor gives us a pool of threads and we can submit tasks to this pool. \n",
    "# The pool would assign tasks to the available threads and schedule them to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ThreadPoolExecutor(4) as executor:\n",
    "    # The map method allows multiple calls to a provided function, \n",
    "    # passing each of the items in an iterable to that function\n",
    "    results = executor.map(urllib.request.urlopen, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ThreadPoolExecutor(8) as executor:\n",
    "    results = executor.map(urllib.request.urlopen, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ThreadPoolExecutor(13) as executor:\n",
    "    results = executor.map(urllib.request.urlopen, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ThreadPoolExecutor(16) as executor:\n",
    "    results = executor.map(urllib.request.urlopen, urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multithreading library is lightweight, shares memory, responsible for responsive UI and is used well for I/O bound applications. \n",
    "\n",
    "Multiple threads live in the same process in the same space, each thread will do a specific task, have its own code, own stack memory, instruction pointer, and share heap memory. \n",
    "\n",
    "Multiple threads can significantly speed up many tasks that are IO-bound. IO-bound programs spend most of the time waiting for input/output. Multithreading is very useful in scenarios like webscrapping. We can do multiple fetches concurrently and processing each page as it returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessing uses multiple processes. \n",
    "\n",
    "Multiprocessing spawns each process with is own interpreter and assigns a separate memory space for it,  so the GIL(Global Interpreter library) does not hold things back. \n",
    "\n",
    "There are 2 main objects in multiprocessing to implement parallel execution of a function: The `Pool` Class and the `Process` Class.\n",
    "\n",
    "If you spawn more processes than what your CPU can handle at a time, you will notice your performance starting to drop. This is because the operating system now has to do more work swapping processes in and out of the CPU cores since you have more processes than cores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see exmample of process class here\n",
    "import os\n",
    "from multiprocessing import Process, current_process\n",
    " \n",
    "# function that will be run in parallel \n",
    "def doubler(number): \n",
    "    \"\"\"\n",
    "    A doubling function that can be used by a process\n",
    "    \"\"\"\n",
    "    result = number * 2\n",
    "    proc_name = current_process().name\n",
    "    print('{0} doubled to {1} by: {2}'.format(\n",
    "        number, result, proc_name))\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    numbers = [5, 10, 15, 20, 25, 100]\n",
    "    procs = []\n",
    "    # Instatiate process object with function name and input data(args)\n",
    "    proc = Process(target=doubler, args=(7,))  \n",
    " \n",
    "    for index, number in enumerate(numbers):\n",
    "        print(\"Begin multiprocessing\")\n",
    "        proc = Process(target=doubler, args=(number,))\n",
    "        procs.append(proc)\n",
    "        # Start the process/function execution\n",
    "        proc.start()  \n",
    "    print('Multi process submission complete')\n",
    " \n",
    "    proc = Process(target=doubler, name='Test', args=(2,))\n",
    "    proc.start()\n",
    "    procs.append(proc)\n",
    " \n",
    "    for proc in procs:\n",
    "        # Tell the parent application that processes are complete.\n",
    "        # Otherswise the processes will remain idle and will not terminate using up resources\n",
    "        proc.join() \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://sebastianraschka.com/Articles/2014_multiprocessing.html\n",
    "* https://www.journaldev.com/15631/python-multiprocessing-example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is an Open Source library that  provides abstractions over NumPy Arrays, Pandas Dataframes and regular lists, allowing you to run operations on them in parallel, using multicore processing.\n",
    "\n",
    "https://docs.dask.org/en/latest/\n",
    "\n",
    "A **Dask DataFrame** is a large parallel DataFrame composed of many smaller Pandas DataFrames, split along the index. These Pandas DataFrames may live on disk for larger-than-memory computing on a single machine, or on many different machines in a cluster. One Dask DataFrame operation triggers many operations on the constituent Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"dask-dataframe.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as ddf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_df = ddf.read_csv('random_people.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask dataframe doesn't know how many records are in your data without first reading through all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.read_csv('random_people.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask_df['new_salary'] = dask_df['salary']**2\n",
    "# dask_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pandas_df['new_salary'] = pandas_df['salary']**2\n",
    "# pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df1 = dask_df[dask_df['new_salary']>5000]\n",
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df2 = pandas_df[pandas_df['new_salary']>5000]\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.salary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask_df.salary.value_counts()\n",
    "dask_df.salary.value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask DataFrames aggregations and apply's are lazy. To trigger computation we need to use `.compute()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features of Dask**\n",
    "* One can use Kubernetes to launch Dask workers\n",
    "* Dask-Yarn deploys Dask on YARN clusters, such as are found in traditional Hadoop installations. Dask-Yarn provides an easy interface to quickly start, scale, and stop Dask clusters natively from Python.\n",
    "* Dask.distributed is a lightweight library for distributed computing in Python. It extends both the concurrent.futures and dask APIs to moderate sized clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://matthewrocklin.com/blog/work/2017/07/03/scaling\n",
    "* https://www.anaconda.com/wp-content/uploads/2019/03/2018-11-Dask_CheatSheet-1.pdf\n",
    "* https://ipython-books.github.io/511-performing-out-of-core-computations-on-large-arrays-with-dask/\n",
    "* https://matthewrocklin.com/blog/work/2017/01/12/dask-dataframes\n",
    "* https://www.youtube.com/watch?v=Q7XyGfS84l0&t=6s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark is the Python API  for Spark. PySpark DataFrames are\n",
    "* **Immutable in nature** : We can create DataFrame once but canâ€™t change it. And we can transform a DataFrame  after applying transformations.\n",
    "* **Lazy Evaluations**: Which means that a task is not executed until an action is performed.\n",
    "* **Distributed**: PySpark DataFrames are distributed in nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(\"random_people.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select('name','salary').show()\n",
    "df.select('name','salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://mapr.com/blog/spark-101-what-it-what-it-does-and-why-it-matters/\n",
    "* https://dzone.com/articles/pyspark-dataframe-tutorial-introduction-to-datafra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
